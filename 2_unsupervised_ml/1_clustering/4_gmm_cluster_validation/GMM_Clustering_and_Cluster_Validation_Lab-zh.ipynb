{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KMeans vs GMM \n",
    "\n",
    "在第一个例子中，我们将生成一个高斯数据集，并尝试对其进行聚类，看看其聚类结果是否与数据集的原始标签相匹配。\n",
    "\n",
    "我们可以使用 sklearn 的 [make_blobs] (http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) 函数来创建高斯 blobs 的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, datasets, mixture\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[5, 1, 0.5],\n",
    "                             random_state=3)\n",
    "X, y = varied[0], varied[1]\n",
    "\n",
    "plt.figure( figsize=(16,12))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, edgecolor='black', lw=1.5, s=100, cmap=plt.get_cmap('viridis'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，当我们把这个数据集交给聚类算法时，我们显然不会传入标签。所以让我们从 k-means 开始，看看它是如何处理这个数据集的。是否会产生与原标签相匹配的聚类？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(16,12))\n",
    "plt.scatter(X[:,0], X[:,1], c=pred, edgecolor='black', lw=1.5, s=100, cmap=plt.get_cmap('viridis'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means 的表现怎么样？它是否能够找到与原始标签匹配或相似的聚类？\n",
    "\n",
    "现在让我们尝试使用 [GaussianMixture](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) 进行聚类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import GaussianMixture\n",
    "from import \n",
    "\n",
    "# TODO: Create an instance of Gaussian Mixture with 3 components\n",
    "gmm = \n",
    "\n",
    "# TODO: fit the dataset\n",
    "gmm = \n",
    "\n",
    "# TODO: predict the clustering labels for the dataset\n",
    "pred_gmm = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plt.figure( figsize=(16,12))\n",
    "plt.scatter(X[:,0], X[:,1], c=pred_gmm, edgecolor='black', lw=1.5, s=100, cmap=plt.get_cmap('viridis'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过视觉比较k-means和GMM聚类的结果，哪一个能更好地匹配原始标签？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. KMeans vs GMM - 鸢尾花(Iris)数据集\n",
    "\n",
    "对于第二个示例，我们将使用一个具有两个以上特征的数据集。鸢尾花(Iris)数据集在这方面做得很好，因为可以合理地假设它的数据分布是高斯分布。\n",
    "\n",
    "鸢尾花(Iris)数据集是一个带标签的数据集，具有四个特征：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有几种方法 (例如 [PairGrid](https://seaborn.pydata.org/generated/seaborn.PairGrid.html), [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), 或 [用 PCA 投影到一个较低的数维](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py))。让我们尝试用 PairGrid 进行可视化，因为它不会扭曲数据集 --它只是在一个子图中将每一对特征进行相互对应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(iris, hue=\"species\", palette=sns.color_palette(\"cubehelix\", 3), vars=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "g.map(plt.scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we cluster the Iris datset using KMeans, how close would the resulting clusters match the original labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_iris = KMeans(n_clusters=3)\n",
    "pred_kmeans_iris = kmeans_iris.fit_predict(iris[['sepal_length','sepal_width','petal_length','petal_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['kmeans_pred'] = pred_kmeans_iris\n",
    "\n",
    "g = sns.PairGrid(iris, hue=\"kmeans_pred\", palette=sns.color_palette(\"cubehelix\", 3), vars=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "g.map(plt.scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these clusters match the original labels?\n",
    "\n",
    "You can clearly see that visual inspection is no longer useful if we're working with multiple dimensions like this. So how can we evaluate the clustering result versus the original labels? \n",
    "\n",
    "You guessed it. We can use an external cluster validation index such as the [adjusted Rand score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) which generates a score between -1 and 1 (where an exact match will be scored as 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import adjusted rand score\n",
    "from import \n",
    "\n",
    "# TODO: calculate adjusted rand score passing in the original labels and the kmeans predicted labels \n",
    "iris_kmeans_score = \n",
    "\n",
    "# Print the score\n",
    "iris_kmeans_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we cluster using Gaussian Mixture models? Would it earn a better ARI score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_iris = GaussianMixture(n_components=3).fit(iris[['sepal_length','sepal_width','petal_length','petal_width']])\n",
    "pred_gmm_iris = gmm_iris.predict(iris[['sepal_length','sepal_width','petal_length','petal_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['gmm_pred'] = pred_gmm_iris\n",
    "\n",
    "# TODO: calculate adjusted rand score passing in the original \n",
    "# labels and the GMM predicted labels iris['species']\n",
    "iris_gmm_score = \n",
    "\n",
    "# Print the score\n",
    "iris_gmm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to ARI socres, we have a clear indicator which clustering result better matches the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
