{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Random Projection\n",
    "In the previous video, you saw an example of working with some MNIST digits data.  A link to the dataset can be found here: http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "First, let's import the necessary libraries.  Notice, there are also some imports from a file called `helper_functions`, which contains the functions used in the previous video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from helper_functions import fit_random_forest_classifier, plot_rp, show_images_by_digit\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Use random projection to reduce dimensions\n",
    "\n",
    "In the first section, we will use sparse random projection to transform data from high dimensions to low dimensions and compare the classification accuracy before and after transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Use pandas to read in the dataset, which can be found at the following address **'./data/train.csv'**.  Take a look at info about the data using `head`, `tail`, `describe`, `info`, etc.  You can learn more about the data values from the article here: https://homepages.inf.ed.ac.uk/rbf/HIPR2/value.htm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Create a vector called **y** that holds the **label** column of the dataset.  Store all other columns holding the pixel data of your images in **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labels to a Pandas series target\n",
    "\n",
    "\n",
    "# Drop the label feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now use the `show_images_by_digit` function from the `helper_functions` module to take a look some of the `1`'s, `2`'s, `3`'s, or any other value you are interested in looking at.  Do they all look like what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_by_digit(6) # Try looking at a few other digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now that you have had a chance to look through some of the data, you can try some different algorithms to see what works well to use the X matrix to predict the response well.  If you would like to use the imported random forests classification, you can run the code below, but you might also try any of the supervised techniques you learned in the previous course to see what works best.\n",
    "\n",
    "If you decide to put together your own classifier, remember the 4 steps to this process:\n",
    "\n",
    "**I.** Instantiate your model. (with all the hyperparameter values you care about)\n",
    "\n",
    "**II.** Fit your model. (to the training data)\n",
    "\n",
    "**III.** Predict using your fitted model.  (on the test data)\n",
    "\n",
    "**IV.** Score your model. (comparing the predictions to the actual values on the test data)\n",
    "\n",
    "You can also try a grid search to see if you can improve on your initial predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_random_forest_classifier(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now the purpose of this lesson, to look at Randome Projection. In the cell below, reduce the dimension of the training data X using the `SparseRandomProjection` in `random_projection` module. You can use `0.5` as the value of `epsilon`. Store your variables in **rp** and **X_rp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: performs random projection to reduce the dataset dimension. \n",
    "# To start, use 0.5 as the epsilon value.\n",
    "\n",
    "from import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the dimension of the transformed data, you can use the `n_components_` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_dim = rp.n_components_\n",
    "X_dim = X.shape[1]\n",
    "print(\"The orignial data has {} dimensions and it is reduced to {} after random projection.\".format(X_dim, rp_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` The **X_rp** has moved about half the original features. Use the space below to fit a model using these two features to predict the written value.  You can use the random forest model by running `fit_random_forest_classifier` the same way as in the video. How well does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: fit the new data with a classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`8.`  Epsilon is the level of errors that determins how much the transformed data is distorded from the original data. Now, see if you can change the epsilon to reduce the dimension even more. What is the accuracy of the classifying model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a loop to transform X using different epsilons and get the accuracy score of classification\n",
    "for sample_eps in np.arange(0.5, stop_value, step_value):\n",
    "    rp =\n",
    "    X_rp = \n",
    "    acc =\n",
    "    print(\"With epsilon = {:.2f}, the transformed data has {} components, a random forest acheived an accuracy of {}.\".format(sample_eps, X_rp.shape[1], acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`9.` As you can see, the accuracy is still very high after recuding more than half of the columns. `epsilon` is an important parameter in the *the [Johnson-Lindenstrauss lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)*. Let's see how epsilon changes the number of components projected and you will notice that a **higer** value of epsilon gives you a dataset with **lower** number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calulate the number of components with varying eps\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "eps = np.arange(0.1, 1, 0.01)\n",
    "n_comp = johnson_lindenstrauss_min_dim(n_samples=1e6, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps, n_comp, 'bo');\n",
    "plt.xlabel('eps');\n",
    "plt.ylabel('Number of Components');\n",
    "plt.title('Number of Components by eps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Visualize the random projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will take a look at how the projection works.\n",
    "\n",
    "`1.` We load the number of samples and number of components in the original data **X**. Store them in `X_sample` and `X_comp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find the number of samples and components in X\n",
    "\n",
    "print(\"The orignial data has {} samples with dimension {}.\".format(X_sample, X_comp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Project the data onto a space with `n_components` using `SparseRamdomProjection`. Sometimes you may want to define the dimension of projected space directly without guessing it based on the epsilon.\n",
    "In random projection, besides using a predefine an epsilon to determine the dimension, you can also use the `n_components` parameter to define the number of components in the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a n_components and perform random projection on data X.\n",
    "# Store the transformed data in a `X_rp` variable\n",
    "n_components = \n",
    "\n",
    "X_rp ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Perform the following helper function to plot two figures. \n",
    "\n",
    "The first figure shows how the distribution of pairwise distance of original data against that of the projected data. \n",
    "\n",
    "The second figure shows the ratio of pairwise distance in projected data and original data (projected distance/original distance)\n",
    "\n",
    "It may take about 2-5 min to plot the figures due to the large data dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rp(X, X_rp, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Change number of `n_components` to 30, 100, 700. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Input your answer here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
